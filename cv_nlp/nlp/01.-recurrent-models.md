---
description: >-
  Transformer 기반의 모델이 등장하기 전까지 각종 NLP task에서 널리 사용되었던 recurrent models (RNN,
  LSTM, GRU)를 설명합니다.
---

# 01. Recurrent Models

## Recurrent Models: Motivation

<figure><img src="../../.gitbook/assets/Recurrent Models motivation.png" alt=""><figcaption><p>Recurrent Models: Motivation</p></figcaption></figure>

기존에 사용되던 multi layer perceptron은 fully connected layer를 여러 개 쌓아서 input vector를 output vector로 mapping해주는 형태의 함수라고 볼 수 있습니다. 고정된 크기의 input을 통해 실수 값을 예측하는 regression 문제나, 여러 개의 class로 분류해주는 classification 문제에 적합한 구조입니다.

그러나 이러한 구조의 모델들은 natural language, audio, video 등 시간에 따른 temporal context를 고려해야 하는 경우에는 적합하지 않습니다. 단순하게 생각해서 어떤 하나의 문장을 실수 값의 벡터로 인코딩한 후, MLP에 집어넣어서 output 문장을 생성한다고 하면 이는 단어들 사이의 순서 관계를 완전히 무시하는 결과가 되고 높은 성능을 내기 힘듭니다. 또한 input, output size가 고정되어 있으므로 scalability의 관점에서도 좋지 않습니다.



## Recurrent Neural Networks

<figure><img src="../../.gitbook/assets/Handling variable sequences.png" alt=""><figcaption><p>Handling variable sequences</p></figcaption></figure>

여기서 핵심은 바로 network architecture가 input sequence에 존재하는 ‘context’를 반영하게 하는 것입니다. 이를 위해서 RNN에서는 hidden state $$t$$를 두고 input이 들어올 때마다 이를 업데이트해주는 방법을 사용합니다. 특정 시점 $$t$$까지 들어온 input들을 어떤 방식으로든 hidden state에 반영하여 특정 시점에서의 output generation에 이를 사용하겠다는 것입니다. 모델이 loop를 돌면서 input sequence에 대한 memory(experience)를 유지하고 활용할 수 있게 된 것입니다.



<figure><img src="../../.gitbook/assets/hidden state update (1).png" alt=""><figcaption><p>Hidden state update</p></figcaption></figure>

구체적으로 보면 input이 들어올 때마다 이전의 hidden state에 weight를 곱해주고, 현재 들어온 input에 또 다른 weight를 곱한 뒤 더해주고 activation function을 통과시켜서 새로운 hidden state를 만들어내게 됩니다.



<figure><img src="../../.gitbook/assets/output generation.png" alt=""><figcaption><p>Output Generation</p></figcaption></figure>

또한 many to many 아키텍쳐의 경우에는 input이 들어올 때마다 그에 해당하는 output을 만들어내야 합니다. 이는 앞서 업데이트된 hidden state $$h_t$$를 또 다른 weight에 곱해서 만들어내면 됩니다. 여기에서 input과 output의 차원이 같게 나와있지만 다르게 만들 수도 있습니다.



<figure><img src="../../.gitbook/assets/ybigta sequence modeling.png" alt=""><figcaption><p>Language Modeling Example</p></figcaption></figure>

조금 더 구체적인 예제를 살펴보겠습니다. RNN을 이용하여 “ybigta”라는 sequence를 modeling하는 예제입니다. 모델에 들어가고 나올 수 있는 character가 6개로 단순하므로 one-hot encoding을 해주었습니다.

각각의 시점에 input vector $$x_t$$를 weight $$W_{xh}$$와 곱해주고, 이전 시점의 hidden state인 $$h_{t - 1}$$을 weight $$W_{hh}$$와 곱해준 뒤 elementwise하게 더해줍니다. 그리고 이를 $$tanh$$ 함수에 통과시켜주면 새로운 hidden state인 $$h_t$$를 구할 수 있게 됩니다.

Hidden state $h\_t$를 구하고 나면 이를 weight $W\_{hy}$와 곱해서 output vector를 만들어낼 수 있습니다. 마지막 character까지 집어넣고 나면 최종 loss를 구한 뒤 이를 backpropagate하여 loss를 최소화하는 방향으로 weight update를 해주면 됩니다.

실제로 language modeling을 할 때는 character 단위로 하기보다는 word 단위로 해야 하므로 word embedding 내용이 포함됩니다. 또한 test time에는 모델이 문장이 언제 끝날지를 알 수 없으므로 EOS (End of Sequence) token을 예측하는 방식으로 training이 진행됩니다.
