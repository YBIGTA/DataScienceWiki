---
description: >-
  Transformer 기반의 모델이 등장하기 전까지 각종 NLP task에서 널리 사용되었던 recurrent models (RNN,
  LSTM, GRU)를 설명합니다.
---

# 01. Recurrent Models

## Recurrent Models: Motivation

<figure><img src="../../.gitbook/assets/Recurrent Models motivation.png" alt=""><figcaption><p>Recurrent Models: Motivation</p></figcaption></figure>

기존에 사용되던 multi layer perceptron은 fully connected layer를 여러 개 쌓아서 input vector를 output vector로 mapping해주는 형태의 함수라고 볼 수 있습니다. 고정된 크기의 input을 통해 실수 값을 예측하는 regression 문제나, 여러 개의 class로 분류해주는 classification 문제에 적합한 구조입니다.

그러나 이러한 구조의 모델들은 natural language, audio, video 등 시간에 따른 temporal context를 고려해야 하는 경우에는 적합하지 않습니다. 단순하게 생각해서 어떤 하나의 문장을 실수 값의 벡터로 인코딩한 후, MLP에 집어넣어서 output 문장을 생성한다고 하면 이는 단어들 사이의 순서 관계를 완전히 무시하는 결과가 되고 높은 성능을 내기 힘듭니다. 또한 input, output size가 고정되어 있으므로 scalability의 관점에서도 좋지 않습니다.



## Recurrent Neural Networks

<figure><img src="../../.gitbook/assets/Handling variable sequences.png" alt=""><figcaption><p>Handling variable sequences</p></figcaption></figure>

여기서 핵심은 바로 network architecture가 input sequence에 존재하는 ‘context’를 반영하게 하는 것입니다. 이를 위해서 RNN에서는 hidden state $$t$$를 두고 input이 들어올 때마다 이를 업데이트해주는 방법을 사용합니다. 특정 시점 $$t$$까지 들어온 input들을 어떤 방식으로든 hidden state에 반영하여 특정 시점에서의 output generation에 이를 사용하겠다는 것입니다. 모델이 loop를 돌면서 input sequence에 대한 memory(experience)를 유지하고 활용할 수 있게 된 것입니다.



<figure><img src="../../.gitbook/assets/hidden state update (1).png" alt=""><figcaption><p>Hidden state update</p></figcaption></figure>

구체적으로 보면 input이 들어올 때마다 이전의 hidden state에 weight를 곱해주고, 현재 들어온 input에 또 다른 weight를 곱한 뒤 더해주고 activation function을 통과시켜서 새로운 hidden state를 만들어내게 됩니다.



<figure><img src="../../.gitbook/assets/output generation.png" alt=""><figcaption><p>ㅒOu</p></figcaption></figure>
